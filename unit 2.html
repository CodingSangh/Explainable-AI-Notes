<html>

<head>
  <link rel="stylesheet" href="style.css">
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <h1 id="unit-ii-interpretable-models-and-intrinsic-xai">Unit II: Interpretable Models and Intrinsic XAI</h1>
  <hr>
  <h2 id="topic-1-white-box-models">Topic 1: White-box models</h2>
  <hr>
  <h3 id="1-introduction-to-intrinsic-xai-white-box-models-">1. Introduction to Intrinsic XAI (White-box Models)</h3>
  <p><strong>Intrinsic XAI</strong> refers to models that are interpretable by design. We do not need a secondary tool
    to explain them; the explanation is embedded in the model&#39;s structure.</p>
  <p>The syllabus covers three main types:</p>
  <ol>
    <li><strong>Linear Models:</strong> (Regression, Logistic Regression) - Interpreted via <strong>Weights</strong>.
    </li>
    <li><strong>Decision Trees:</strong> Interpreted via <strong>Rules/Paths</strong>.</li>
    <li><strong>Rule-based Learners:</strong> Interpreted via <strong>If-Then Logic</strong>.</li>
  </ol>
  <hr>
  <h3 id="2-linear-regression-the-math-of-interpretability">2. Linear Regression: The Math of Interpretability</h3>
  <p>Linear Regression is the &quot;Hello World&quot; of Machine Learning and the gold standard for interpretability.
  </p>
  <h4 id="a-the-mathematical-formulation">A. The Mathematical Formulation</h4>
  <p>The model predicts a target $y$ based on inputs $x_1, x_2, ... x_n$ using a weighted sum:</p>
  <p>$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon$$</p>
  <ul>
    <li>$y$: The prediction (e.g., House Price).</li>
    <li>$\beta_0$: The <strong>Intercept</strong> (Bias). The baseline value when all inputs are 0.</li>
    <li>$\beta_i$: The <strong>Coefficients</strong> (Weights). This is the core of the explanation.</li>
    <li>$x_i$: The Input features (e.g., Square footage, Number of rooms).</li>
  </ul>
  <h4 id="b-how-to-interpret-the-explanation-">B. How to Interpret (The &quot;Explanation&quot;)</h4>
  <p>In Linear Regression, the <strong>Coefficient ($\beta$)</strong> tells you exactly how the feature affects the
    output.</p>
  <ul>
    <li><strong>Sign (+/-):</strong>
      <ul>
        <li>Positive $\beta$: As $x$ increases, $y$ increases. (e.g., Size $\uparrow$ Price $\uparrow$).</li>
        <li>Negative $\beta$: As $x$ increases, $y$ decreases. (e.g., Age of house $\uparrow$ Price $\downarrow$).</li>
      </ul>
    </li>
    <li><strong>Magnitude:</strong>
      <ul>
        <li>A larger absolute value $|\beta|$ means the feature has a stronger impact on the prediction.</li>
      </ul>
    </li>
  </ul>
  <blockquote>
    <p><strong>The &quot;10-Mark&quot; Explanation Statement:</strong>
      &quot;For every 1-unit increase in feature $x_1$, the output $y$ changes by exactly $\beta_1$, assuming all other
      variables remain constant.&quot;</p>
  </blockquote>
  <h4 id="c-visualizing-linear-interpretability">C. Visualizing Linear Interpretability</h4>
  <p>[Image of linear regression best fit line with residuals]</p>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <line x1="50" y1="250" x2="500" y2="250" stroke="black" stroke-width="2" />
    <line x1="50" y1="250" x2="50" y2="50" stroke="black" stroke-width="2" />
    <text x="500" y="270" text-anchor="end" font-style="italic">House Size (sq ft)</text>
    <text x="40" y="50" text-anchor="end" font-style="italic" transform="rotate(-90 40,50)">Price ($)</text>

    <circle cx="100" cy="220" r="4" fill="#3498db" />
    <circle cx="150" cy="180" r="4" fill="#3498db" />
    <circle cx="200" cy="190" r="4" fill="#3498db" />
    <circle cx="300" cy="120" r="4" fill="#3498db" />
    <circle cx="400" cy="80" r="4" fill="#3498db" />

    <line x1="50" y1="240" x2="450" y2="60" stroke="#e74c3c" stroke-width="3" />
    <text x="350" y="70" fill="#e74c3c" font-weight="bold">y = βx + c</text>

    <line x1="250" y1="150" x2="300" y2="150" stroke="black" stroke-width="1" stroke-dasharray="4" />
    <line x1="300" y1="150" x2="300" y2="128" stroke="black" stroke-width="1" stroke-dasharray="4" />
    <text x="260" y="165" font-size="12">Δx (1 unit)</text>
    <text x="310" y="145" font-size="12" fill="#e74c3c">Δy = β</text>
  </svg>

  <hr>
  <h3 id="3-logistic-regression-interpreting-probability">3. Logistic Regression: Interpreting Probability</h3>
  <p>While Linear Regression predicts values (Price), Logistic Regression predicts <strong>Categories</strong> (Yes/No,
    Spam/Not Spam).</p>
  <h4 id="a-the-math">A. The Math</h4>
  <p>It uses the <strong>Sigmoid Function</strong> to squash the output between 0 and 1.
    $$P(y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ...)}}$$</p>
  <h4 id="b-interpretation-odds-ratio-">B. Interpretation (Odds Ratio)</h4>
  <p>This is trickier than linear regression. We cannot say &quot;If $x$ increases by 1, probability increases by
    $\beta$.&quot;
    Instead, we interpret the <strong>Odds</strong>:</p>
  <ul>
    <li><strong>Equation:</strong> $\ln(\frac{P}{1-P}) = \beta_0 + \beta_1 x$</li>
    <li><strong>The Interpretation:</strong> Increasing $x_1$ by 1 unit increases the <strong>log-odds</strong> of the
      outcome by $\beta_1$.<ul>
        <li>If $e^{\beta_1} &gt; 1$: The odds increase.</li>
        <li>If $e^{\beta_1} &lt; 1$: The odds decrease.</li>
      </ul>
    </li>
  </ul>
  <blockquote>
    <p><strong>Example:</strong> Predicting &quot;Heart Disease.&quot;
      If the coefficient for &quot;Cholesterol&quot; is $0.5$, then increasing Cholesterol by 1 unit increases the
      <em>odds</em> of heart disease by factor $e^{0.5} \approx 1.65$. (65% higher odds).</p>
  </blockquote>
  <hr>
  <h3 id="4-advantages-and-limitations-intrinsic-models-">4. Advantages and Limitations (Intrinsic Models)</h3>
  <p>This is a guaranteed exam question.</p>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Feature</th>
        <th style="text-align:left">Advantages</th>
        <th style="text-align:left">Limitations</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Interpretability</strong></td>
        <td style="text-align:left"><strong>Perfect.</strong> We know exactly how every feature contributes.</td>
        <td style="text-align:left"><strong>None</strong> in terms of clarity, but complexity kills it.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Computational</strong></td>
        <td style="text-align:left"><strong>Fast.</strong> Training is instant; prediction is just multiplication.</td>
        <td style="text-align:left"><strong>Curse of Dimensionality.</strong> Performance drops with too many features.
        </td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Accuracy</strong></td>
        <td style="text-align:left">Good for simple, linear problems.</td>
        <td style="text-align:left"><strong>Poor</strong> for non-linear problems (Images, Speech, Complex patterns).
        </td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Causality</strong></td>
        <td style="text-align:left">Helps infer causal relationships (e.g., Smoking -&gt; Cancer).</td>
        <td style="text-align:left"><strong>Correlation $\neq$ Causation.</strong> It assumes independence between
          features (Multicollinearity kills interpretability).</td>
      </tr>
    </tbody>
  </table>
  <h4 id="the-problem-of-multicollinearity">The Problem of Multicollinearity</h4>
  <p>If two features are highly correlated (e.g., <code>Left_Shoe_Size</code> and <code>Right_Shoe_Size</code>), the
    linear model gets confused. It might give a huge positive weight to the Left Shoe and a huge negative weight to the
    Right Shoe to balance them out.</p>
  <ul>
    <li><strong>Result:</strong> The model works, but the <strong>Explanation is broken</strong>.</li>
  </ul>
  <hr>
  <h3 id="5-exam-question-bank">5. Exam Question Bank</h3>
  <p><strong>Q1 (Short - 2 Marks): Why is Linear Regression considered a White-box model?</strong></p>
  <blockquote>
    <p><strong>Ans:</strong> Because the relationship between input and output is defined by explicit coefficients
      ($\beta$). The magnitude and sign of these coefficients directly quantify the importance and effect of each
      feature.</p>
  </blockquote>
  <p><strong>Q2 (Long - 10 Marks): Explain the mathematical formulation of Linear Regression. Discuss its advantages and
      limitations in the context of XAI.</strong></p>
  <blockquote>
    <p><strong>Hint:</strong></p>
    <ol>
      <li>Write the equation $y = \beta x + c$.</li>
      <li>Explain what $\beta$ means (The weight/importance).</li>
      <li><strong>Advantages:</strong> Transparency, speed, causal inference.</li>
      <li><strong>Limitations:</strong> Fails on non-linear data (draw a curve that a straight line cannot fit),
        sensitive to outliers, suffers from multicollinearity.</li>
    </ol>
  </blockquote>
  <hr>
  <h2 id="topic-2-decision-trees">Topic 2: Decision Trees</h2>
  <hr>
  <h3 id="1-introduction-the-flowchart-model">1. Introduction: The Flowchart Model</h3>
  <p>A <strong>Decision Tree</strong> is a hierarchical structure that breaks down a dataset into smaller and smaller
    subsets while at the same time an associated decision tree is incrementally developed.</p>
  <ul>
    <li><strong>Analogy:</strong> It works like a game of &quot;20 Questions.&quot; You ask a broad question (&quot;Is
      it an animal?&quot;), then a specific one (&quot;Does it bark?&quot;), until you reach a conclusion
      (&quot;It&#39;s a dog&quot;).</li>
    <li><strong>Why it is White-box:</strong> You can print the tree on paper and trace the exact path the AI took to
      reach a conclusion.</li>
  </ul>
  <hr>
  <h3 id="2-structure-of-a-decision-tree">2. Structure of a Decision Tree</h3>
  <p>A tree consists of three types of nodes:</p>
  <ol>
    <li><strong>Root Node:</strong> The top-most node. It represents the entire population and gets split based on the
      <strong>most important feature</strong>.</li>
    <li><strong>Decision Nodes (Internal Nodes):</strong> Sub-nodes that split into further sub-nodes.</li>
    <li><strong>Leaf Nodes (Terminal Nodes):</strong> Nodes that do not split. They hold the final prediction (Class
      Label).</li>
  </ol>
  <h4 id="visualizing-the-logic">Visualizing the Logic</h4>
  <svg width="100%" height="350" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <rect x="250" y="20" width="120" height="50" fill="#e3f2fd" stroke="#2196f3" rx="5" />
    <text x="310" y="45" text-anchor="middle" font-weight="bold">Age &gt; 30?</text>
    <text x="310" y="65" text-anchor="middle" font-size="10" fill="#555">(Root Node)</text>

    <line x1="310" y1="70" x2="150" y2="120" stroke="#333" stroke-width="2" />
    <text x="210" y="90" fill="#27ae60" font-weight="bold">Yes</text>

    <line x1="310" y1="70" x2="470" y2="120" stroke="#333" stroke-width="2" />
    <text x="410" y="90" fill="#c0392b" font-weight="bold">No</text>

    <rect x="90" y="120" width="120" height="50" fill="#fff3e0" stroke="#e67e22" rx="5" />
    <text x="150" y="145" text-anchor="middle" font-weight="bold">Salary &gt; 50k?</text>

    <rect x="410" y="120" width="120" height="50" fill="#e8f5e9" stroke="#4caf50" rx="5" />
    <text x="470" y="150" text-anchor="middle" font-weight="bold" fill="#2e7d32">Reject Loan</text>
    <text x="470" y="165" text-anchor="middle" font-size="10" fill="#555">(Leaf Node)</text>

    <line x1="150" y1="170" x2="80" y2="230" stroke="#333" stroke-width="2" />
    <text x="100" y="200" fill="#27ae60" font-size="10">Yes</text>

    <line x1="150" y1="170" x2="220" y2="230" stroke="#333" stroke-width="2" />
    <text x="200" y="200" fill="#c0392b" font-size="10">No</text>

    <rect x="20" y="230" width="120" height="50" fill="#e8f5e9" stroke="#4caf50" rx="5" />
    <text x="80" y="260" text-anchor="middle" font-weight="bold" fill="#2e7d32">Approve Loan</text>

    <rect x="160" y="230" width="120" height="50" fill="#ffebee" stroke="#c62828" rx="5" />
    <text x="220" y="260" text-anchor="middle" font-weight="bold" fill="#c62828">Check Credit</text>
  </svg>

  <hr>
  <h3 id="3-the-math-behind-the-splits-how-it-learns-">3. The Math Behind the Splits (How it Learns)</h3>
  <p>The tree doesn&#39;t just guess. It uses math to decide which feature puts the data into the &quot;purest&quot;
    groups.</p>
  <h4 id="a-entropy-and-information-gain">A. Entropy and Information Gain</h4>
  <ul>
    <li><strong>Entropy ($H$):</strong> A measure of disorder or impurity.<ul>
        <li>If a bucket has 50 red balls and 50 blue balls, Entropy is <strong>High (1.0)</strong> (Maximum confusion).
        </li>
        <li>If a bucket has 100 red balls, Entropy is <strong>Low (0.0)</strong> (Pure).</li>
        <li><strong>Formula:</strong> $$H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)$$</li>
      </ul>
    </li>
    <li><strong>Information Gain:</strong> The reduction in entropy after splitting a dataset on an attribute. The tree
      chooses the feature with the <strong>Highest Information Gain</strong>.</li>
  </ul>
  <h4 id="b-gini-impurity-used-in-cart-algorithm-">B. Gini Impurity (Used in CART algorithm)</h4>
  <ul>
    <li>A simpler alternative to Entropy. It measures how often a randomly chosen element would be incorrectly labeled.
    </li>
    <li><strong>Formula:</strong> $$Gini = 1 - \sum_{i=1}^{c} (p_i)^2$$</li>
    <li><strong>Goal:</strong> The tree wants to minimize Gini Impurity.</li>
  </ul>
  <hr>
  <h3 id="4-feature-importance-in-trees">4. Feature Importance in Trees</h3>
  <p>One of the best XAI features of trees is <strong>Global Feature Importance</strong>.</p>
  <ul>
    <li><strong>How it works:</strong> Features that appear higher up in the tree (near the Root) are more important.
      They cause the biggest split in data.</li>
    <li><strong>Interpretation:</strong> If &quot;Credit Score&quot; is the Root Node, it is globally the most critical
      factor for the decision.</li>
  </ul>
  <hr>
  <h3 id="5-advantages-and-disadvantages">5. Advantages and Disadvantages</h3>
  <p>This is a critical section for exam comparisons.</p>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Feature</th>
        <th style="text-align:left">Advantages (The Good)</th>
        <th style="text-align:left">Disadvantages (The Bad)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Interpretability</strong></td>
        <td style="text-align:left"><strong>Excellent.</strong> Can be visualized as rules. Even non-experts understand
          &quot;If-Then.&quot;</td>
        <td style="text-align:left"><strong>Tree Depth:</strong> If the tree gets too deep (e.g., 100 layers), it
          becomes a &quot;Spaghetti&quot; mess and loses interpretability.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Data Types</strong></td>
        <td style="text-align:left">Handles both Numerical (Age) and Categorical (Gender) data well.</td>
        <td style="text-align:left"><strong>Instability:</strong> A small change in the data can result in a completely
          different tree structure.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Non-linearity</strong></td>
        <td style="text-align:left">Can model complex, non-linear boundaries (unlike Linear Regression).</td>
        <td style="text-align:left"><strong>Overfitting:</strong> Trees tend to memorize the training data perfectly,
          failing on new data unless pruned (Regularization).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Preprocessing</strong></td>
        <td style="text-align:left">Requires little data preparation (no normalization needed).</td>
        <td style="text-align:left"><strong>Bias:</strong> Can be biased towards features with more levels (e.g.,
          &quot;Zip Code&quot; vs &quot;Gender&quot;).</td>
      </tr>
    </tbody>
  </table>
  <hr>
  <h3 id="6-exam-question-bank">6. Exam Question Bank</h3>
  <p><strong>Q1 (2 Marks): What is the &quot;Root Node&quot; in a Decision Tree?</strong></p>
  <blockquote>
    <p><strong>Ans:</strong> The Root Node is the topmost node of the tree. It represents the entire dataset and
      corresponds to the single best feature that splits the data into the most homogeneous (pure) groups.</p>
  </blockquote>
  <p><strong>Q2 (10 Marks): Explain the working mechanism of Decision Trees. How is feature importance calculated, and
      why are they considered interpretable?</strong></p>
  <blockquote>
    <p><strong>Structure:</strong></p>
    <ol>
      <li><strong>Define:</strong> Hierarchical model using splitting rules.</li>
      <li><strong>Mechanism:</strong> Explain Root -&gt; Split -&gt; Leaf. Mention <strong>Entropy</strong> or
        <strong>Gini Index</strong> as the math behind the choice.</li>
      <li><strong>Feature Importance:</strong> Explain that top nodes = high importance.</li>
      <li><strong>Interpretability:</strong> Draw the flowchart diagram. State that &quot;If-Then&quot; rules are easy
        for humans to verify (Transparent).</li>
      <li><strong>Critique:</strong> Mention that very deep trees become uninterpretable (The &quot;Black-box&quot;
        boundary).</li>
    </ol>
  </blockquote>
  <hr>
  <h2 id="topic-3-rule-based-learners">Topic 3: Rule-based Learners</h2>
  <hr>
  <h3 id="1-introduction-learning-by-instruction">1. Introduction: Learning by Instruction</h3>
  <p><strong>Rule-based Learners</strong> are machine learning models that extract knowledge from data in the form of
    easy-to-understand <strong>IF-THEN</strong> rules.</p>
  <ul>
    <li><strong>Format:</strong> <code>IF (Condition A) AND (Condition B) THEN (Prediction)</code></li>
    <li><strong>Terminology:</strong>
      <ul>
        <li><strong>Antecedent (Left-hand side):</strong> The <code>IF</code> part (Conditions).</li>
        <li><strong>Consequent (Right-hand side):</strong> The <code>THEN</code> part (Outcome).</li>
      </ul>
    </li>
  </ul>
  <p><strong>Why is this &quot;Interpretable&quot;?</strong>
    Unlike a neural network which is a block of math, a rule set reads like a manual. A doctor can look at a rule set
    and say, &quot;Yes, this rule makes medical sense.&quot;</p>
  <hr>
  <h3 id="2-algorithms-for-rule-learning">2. Algorithms for Rule Learning</h3>
  <p>There are two main ways to generate these rules:</p>
  <h4 id="a-oner-one-rule-">A. OneR (One Rule)</h4>
  <p>This is the simplest possible classification algorithm.</p>
  <ul>
    <li><strong>Logic:</strong> It generates one rule for each predictor in the data, then selects the <strong>single
        rule with the smallest total error</strong>.</li>
    <li><strong>Philosophy:</strong> &quot;Just find the one feature that matters most.&quot;</li>
    <li><strong>Example:</strong> In a dataset predicting &quot;Will play tennis?&quot;, OneR might find that
      <code>Outlook</code> is the best feature.<ul>
        <li><em>Rule:</em>
          <code>IF Outlook = Sunny THEN No; IF Outlook = Overcast THEN Yes; IF Outlook = Rainy THEN Yes.</code></li>
      </ul>
    </li>
  </ul>
  <h4 id="b-sequential-covering-separate-and-conquer-">B. Sequential Covering (Separate-and-Conquer)</h4>
  <p>This is a more advanced iterative algorithm (used in algorithms like <strong>RIPPER</strong>).</p>
  <ol>
    <li><strong>Step 1:</strong> Find a rule that covers a part of the positive examples (e.g., &quot;IF Smoker=True
      THEN Cancer=High&quot;).</li>
    <li><strong>Step 2:</strong> <strong>Remove</strong> all the data points covered by this rule from the dataset.</li>
    <li><strong>Step 3:</strong> Repeat the process on the remaining data to find the next rule.</li>
    <li><strong>Step 4:</strong> Stop when quality drops or no data is left.</li>
  </ol>
  <h4 id="visualizing-sequential-covering">Visualizing Sequential Covering</h4>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <circle cx="80" cy="80" r="5" fill="#e74c3c" />
    <circle cx="100" cy="60" r="5" fill="#e74c3c" />
    <circle cx="120" cy="90" r="5" fill="#e74c3c" />

    <circle cx="350" cy="220" r="5" fill="#e74c3c" />
    <circle cx="380" cy="200" r="5" fill="#e74c3c" />
    <circle cx="360" cy="240" r="5" fill="#e74c3c" />

    <circle cx="200" cy="150" r="5" fill="#3498db" />
    <circle cx="250" cy="100" r="5" fill="#3498db" />
    <circle cx="150" cy="250" r="5" fill="#3498db" />

    <rect x="50" y="40" width="100" height="80" fill="none" stroke="#27ae60" stroke-width="3" stroke-dasharray="5,5" />
    <text x="60" y="30" fill="#27ae60" font-weight="bold">Rule 1 Covers These</text>

    <rect x="320" y="180" width="100" height="80" fill="none" stroke="#8e44ad" stroke-width="3"
      stroke-dasharray="5,5" />
    <text x="320" y="170" fill="#8e44ad" font-weight="bold">Rule 2 Covers These</text>

    <text x="50" y="280" font-size="12" fill="#555">1. Find Rule 1 -&gt; Remove Red Dots inside.</text>
    <text x="50" y="295" font-size="12" fill="#555">2. Find Rule 2 for remaining Red Dots.</text>
  </svg>

  <hr>
  <h3 id="3-decision-trees-vs-rule-lists">3. Decision Trees vs. Rule Lists</h3>
  <p>This is a common interview and exam comparison.</p>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Feature</th>
        <th style="text-align:left">Decision Trees</th>
        <th style="text-align:left">Rule Lists</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Structure</strong></td>
        <td style="text-align:left">Hierarchical (Flowchart).</td>
        <td style="text-align:left">Flat List (Decision List).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Reading Flow</strong></td>
        <td style="text-align:left">You must trace from Root -&gt; Leaf.</td>
        <td style="text-align:left">You read rules top-to-bottom.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Complexity</strong></td>
        <td style="text-align:left">Can become &quot;bushy&quot; and hard to print.</td>
        <td style="text-align:left">Usually concise (if regularized).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Logic</strong></td>
        <td style="text-align:left"><strong>Mutual Exclusion:</strong> Every data point falls into exactly one leaf.
        </td>
        <td style="text-align:left"><strong>Overlap:</strong> A data point might satisfy multiple rules (ordered lists
          handle this by taking the first match).</td>
      </tr>
    </tbody>
  </table>
  <hr>
  <h3 id="4-advantages-and-limitations">4. Advantages and Limitations</h3>
  <p><strong>Advantages:</strong></p>
  <ol>
    <li><strong>Extreme Interpretability:</strong> &quot;If Age &gt; 60 then Risk = High&quot; is understandable by
      anyone.</li>
    <li><strong>Compactness:</strong> Rule lists are often shorter than their equivalent decision trees.</li>
    <li><strong>Feature selection:</strong> They automatically ignore irrelevant features.</li>
  </ol>
  <p><strong>Limitations:</strong></p>
  <ol>
    <li><strong>Instability:</strong> Small changes in training data can lead to a completely different set of rules.
    </li>
    <li><strong>Categorical Bias:</strong> Like trees, they prefer features with many categories.</li>
    <li><strong>Performance:</strong> Generally less accurate than &quot;Black Box&quot; models like Gradient Boosting.
    </li>
  </ol>
  <hr>
  <h3 id="5-exam-question-bank">5. Exam Question Bank</h3>
  <p><strong>Q1 (2 Marks): What is the &quot;Antecedent&quot; and &quot;Consequent&quot; in a rule-based
      learner?</strong></p>
  <blockquote>
    <p><strong>Ans:</strong> In a rule <code>IF A THEN B</code>:</p>
    <ul>
      <li><strong>Antecedent:</strong> The condition part (<code>A</code>), also called the body or premise.</li>
      <li><strong>Consequent:</strong> The prediction part (<code>B</code>), also called the head or conclusion.</li>
    </ul>
  </blockquote>
  <p><strong>Q2 (5 Marks): Explain the &quot;Separate-and-Conquer&quot; strategy in rule learning.</strong></p>
  <blockquote>
    <p><strong>Hint:</strong> Describe the iterative process:</p>
    <ol>
      <li>Find a rule that explains a subset of positive examples.</li>
      <li>Remove those examples from the dataset.</li>
      <li>Repeat until no positive examples remain.
        <em>Draw the circle diagram showing data being &quot;eaten&quot; by rules.</em>
      </li>
    </ol>
  </blockquote>
  <p><strong>Q3 (10 Marks): Compare Decision Trees and Rule-based Learners in terms of interpretability and
      structure.</strong></p>
  <blockquote>
    <p><strong>Hint:</strong> Use the table provided in Section 3. Highlight that Trees are hierarchical (good for
      visualizing the whole logic), while Rule Lists are flat (good for quick lookups).</p>
  </blockquote>
  <hr>
  <h2 id="topic-4-feature-importance-in-interpretable-models">Topic 4: Feature Importance in Interpretable Models</h2>
  <hr>
  <h3 id="1-introduction-what-is-feature-importance-">1. Introduction: What is Feature Importance?</h3>
  <p><strong>Feature Importance</strong> refers to techniques that assign a score to input features (variables) based on
    how useful they are at predicting a target variable.</p>
  <ul>
    <li><strong>The Goal:</strong> To answer the question, <em>&quot;Which factors drove the decision?&quot;</em></li>
    <li><strong>The Output:</strong> A ranked list.<ul>
        <li><em>Example (Predicting House Prices):</em>
          <ol>
            <li>Square Footage (Importance: 0.8)</li>
            <li>Location (Importance: 0.6)</li>
            <li>Color of Walls (Importance: 0.01 - Irrelevant)</li>
          </ol>
        </li>
      </ul>
    </li>
  </ul>
  <p><strong>Why is this critical for XAI?</strong>
    It helps in <strong>Feature Selection</strong> (removing noise) and <strong>Trust Building</strong> (verifying that
    the model isn&#39;t using cheating features like ID numbers).</p>
  <hr>
  <h3 id="2-feature-importance-in-linear-models-regression-">2. Feature Importance in Linear Models (Regression)</h3>
  <p>In linear models, feature importance is derived directly from the <strong>Coefficients</strong> (Weights).</p>
  <h4 id="a-the-math">A. The Math</h4>
  <p>$$y = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n$$</p>
  <ul>
    <li>The importance of feature $x_i$ is proportional to the <strong>absolute value</strong> of its coefficient
      $|\beta_i|$.</li>
    <li><strong>Interpretation:</strong> A high positive $\beta$ means the feature strongly pushes the prediction
      <em>up</em>. A high negative $\beta$ means it strongly pushes the prediction <em>down</em>.</li>
  </ul>
  <h4 id="b-the-crucial-caveat-normalization">B. The Crucial Caveat: Normalization</h4>
  <p>You <strong>cannot</strong> compare coefficients directly if the features have different scales.</p>
  <ul>
    <li><em>Scenario:</em>
      <ul>
        <li>Feature A: <code>Income</code> (Range: 0 - 100,000) $\to \beta_A = 0.0001$</li>
        <li>Feature B: <code>Age</code> (Range: 0 - 100) $\to \beta_B = 2.5$</li>
      </ul>
    </li>
    <li><em>False Conclusion:</em> Age is more important because $2.5 &gt; 0.0001$.</li>
    <li><em>Reality:</em> Because Income has huge values, its small weight might still result in a massive impact.</li>
    <li><strong>The Fix:</strong> You must <strong>Standardize</strong> (Scale) data so all features have Mean=0 and
      Variance=1 before comparing $\beta$.</li>
  </ul>
  <hr>
  <h3 id="3-feature-importance-in-decision-trees">3. Feature Importance in Decision Trees</h3>
  <p>In trees, importance is calculated differently. We don&#39;t have coefficients. Instead, we use <strong>Impurity
      Reduction</strong>.</p>
  <h4 id="a-mean-decrease-in-impurity-mdi-">A. Mean Decrease in Impurity (MDI)</h4>
  <ul>
    <li><strong>Concept:</strong> Every time a tree splits on a feature (e.g., &quot;Is Smoker?&quot;), the impurity
      (entropy) of the data decreases.</li>
    <li><strong>Calculation:</strong>
      <ol>
        <li>For each feature, sum up the total reduction in Gini Impurity (or Entropy) across all nodes where that
          feature is used as a splitter.</li>
        <li>Weighted by the number of samples passing through that node.</li>
      </ol>
    </li>
    <li><strong>Result:</strong> Features used at the <strong>Top (Root)</strong> of the tree generally have the highest
      importance because they split the largest chunk of data.</li>
  </ul>
  <p>[Image of decision tree feature importance plot]</p>
  <h4 id="b-visualization-bar-plot-">B. Visualization (Bar Plot)</h4>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <text x="50" y="30" font-family="Arial" font-size="16" font-weight="bold">Global Feature Importance (Random
      Forest)</text>

    <text x="120" y="80" text-anchor="end" font-family="Arial" font-size="12">Income</text>
    <text x="120" y="130" text-anchor="end" font-family="Arial" font-size="12">Credit Score</text>
    <text x="120" y="180" text-anchor="end" font-family="Arial" font-size="12">Debt Ratio</text>
    <text x="120" y="230" text-anchor="end" font-family="Arial" font-size="12">Zip Code</text>

    <rect x="130" y="60" width="300" height="30" fill="#3498db" rx="3" />
    <text x="440" y="80" font-size="12" fill="#555">0.85</text>

    <rect x="130" y="110" width="200" height="30" fill="#3498db" rx="3" />
    <text x="340" y="130" font-size="12" fill="#555">0.55</text>

    <rect x="130" y="160" width="80" height="30" fill="#3498db" rx="3" />
    <text x="220" y="180" font-size="12" fill="#555">0.20</text>

    <rect x="130" y="210" width="10" height="30" fill="#95a5a6" rx="3" />
    <text x="150" y="230" font-size="12" fill="#555">0.02</text>

    <line x1="130" y1="50" x2="130" y2="260" stroke="#333" stroke-width="2" />
    <line x1="130" y1="260" x2="480" y2="260" stroke="#333" stroke-width="2" />
  </svg>

  <hr>
  <h3 id="4-comparison-linear-vs-tree-importance">4. Comparison: Linear vs. Tree Importance</h3>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Feature</th>
        <th style="text-align:left">Linear Models ($\beta$)</th>
        <th style="text-align:left">Decision Trees (MDI)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Direction</strong></td>
        <td style="text-align:left">Indicates <strong>Direction</strong> (+/-). We know if the feature helps or hurts.
        </td>
        <td style="text-align:left"><strong>Magnitude Only.</strong> We know it&#39;s important, but not if it
          increases/decreases the outcome.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Relationships</strong></td>
        <td style="text-align:left">Assumes <strong>Linear</strong> relationships.</td>
        <td style="text-align:left">Captures <strong>Non-linear</strong> and interaction effects.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Scale Sensitivity</strong></td>
        <td style="text-align:left">Highly sensitive (Requires Scaling).</td>
        <td style="text-align:left">Invariant to scale (No normalization needed).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Robustness</strong></td>
        <td style="text-align:left">Fails with Collinearity.</td>
        <td style="text-align:left">Better with interactions, but can be biased toward high-cardinality features.</td>
      </tr>
    </tbody>
  </table>
  <hr>
  <h3 id="5-the-trap-of-correlated-features-exam-warning-">5. The Trap of Correlated Features (Exam Warning)</h3>
  <p><strong>The Problem:</strong>
    If two features are identical (e.g., <code>Temperature_Celsius</code> and <code>Temperature_Fahrenheit</code>):</p>
  <ul>
    <li><strong>Linear Model:</strong> Might split the weight between them (0.5 each) or give one 1.0 and the other 0.0.
      This dilutes the &quot;True&quot; importance.</li>
    <li><strong>Tree Model:</strong> Will pick one randomly to split on. The importance of the other drops to zero.</li>
  </ul>
  <p><strong>Conclusion:</strong> Feature Importance is unreliable if you have highly correlated features
    (Multicollinearity). You must remove duplicates first.</p>
  <hr>
  <h3 id="6-exam-question-bank">6. Exam Question Bank</h3>
  <p><strong>Q1 (Short - 2 Marks): What is the condition required to interpret Linear Regression coefficients as feature
      importance?</strong></p>
  <blockquote>
    <p><strong>Ans:</strong> The input features must be <strong>scaled (normalized/standardized)</strong> to the same
      range. Otherwise, features with larger numerical values (like Salary) will artificially have smaller coefficients
      than features with small values (like Age).</p>
  </blockquote>
  <p><strong>Q2 (10 Marks): Compare Feature Importance in Linear Models versus Decision Trees.</strong></p>
  <blockquote>
    <p><strong>Hint:</strong></p>
    <ol>
      <li><strong>Linear:</strong> Uses Coefficients ($\beta$). Pros: Shows direction (+/-). Cons: Needs scaling, fails
        on non-linear data.</li>
      <li><strong>Trees:</strong> Uses Impurity Reduction (Gini/Entropy). Pros: Handles non-linear data, no scaling
        needed. Cons: Doesn&#39;t show direction (Positive/Negative impact).</li>
      <li><strong>Visualization:</strong> Sketch the bar chart shown above.</li>
    </ol>
  </blockquote>
  <hr>
  <h2 id="topic-5-global-vs-local-interpretability-regularization">Topic 5: Global vs. Local Interpretability &amp;
    Regularization</h2>
  <hr>
  <h3 id="1-global-vs-local-interpretability">1. Global vs. Local Interpretability</h3>
  <p>When explaining a model, the first question is: &quot;Are we explaining the <em>whole</em> model or just
    <em>one</em> decision?&quot;</p>
  <h4 id="a-global-interpretability-the-bird-s-eye-view-">A. Global Interpretability (The &quot;Bird&#39;s Eye
    View&quot;)</h4>
  <ul>
    <li><strong>Definition:</strong> Understanding how the model makes decisions <strong>on average</strong> across the
      entire dataset. It reveals the general logic and rules of the system.</li>
    <li><strong>Key Question:</strong> &quot;What features does the model consider most important overall?&quot;</li>
    <li><strong>Example:</strong>
      <ul>
        <li><em>Model:</em> A Credit Scoring system.</li>
        <li><em>Global Explanation:</em> &quot;Overall, this model relies 60% on Credit History and 30% on Salary. It
          ignores Age.&quot;</li>
      </ul>
    </li>
    <li><strong>Tools:</strong> Feature Importance plots (from Random Forests), Linear Weights ($\beta$), Decision Tree
      logic.</li>
  </ul>
  <h4 id="b-local-interpretability-the-microscope-view-">B. Local Interpretability (The &quot;Microscope View&quot;)
  </h4>
  <ul>
    <li><strong>Definition:</strong> Understanding why the model made a specific prediction for a <strong>single
        instance</strong> (a specific person or image).</li>
    <li><strong>Key Question:</strong> &quot;Why was <strong>Mr. Smith&#39;s</strong> loan rejected?&quot;</li>
    <li><strong>Example:</strong>
      <ul>
        <li><em>Model:</em> The same Credit Scoring system.</li>
        <li><em>Local Explanation:</em> &quot;For Mr. Smith, the model rejected the loan because his recent &#39;Late
          Payments&#39; count was too high, even though his Salary is high.&quot;</li>
      </ul>
    </li>
    <li><strong>Tools:</strong> LIME, SHAP (Local plots), Counterfactuals.</li>
  </ul>
  <h4 id="visual-comparison">Visual Comparison</h4>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <text x="150" y="30" font-family="Arial" font-size="16" font-weight="bold" text-anchor="middle"
      fill="#2c3e50">Global Interpretability</text>
    <image
      href="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/World_map_blank_without_borders.svg/1000px-World_map_blank_without_borders.svg.png"
      x="80" y="50" width="140" height="80" opacity="0.2" />
    <rect x="50" y="50" width="200" height="150" fill="none" stroke="#3498db" stroke-width="2" rx="5" />

    <rect x="70" y="70" width="100" height="20" fill="#3498db" /> <text x="180" y="85" font-size="10">Feature A</text>
    <rect x="70" y="100" width="60" height="20" fill="#3498db" /> <text x="180" y="115" font-size="10">Feature B</text>
    <rect x="70" y="130" width="150" height="20" fill="#3498db" /> <text x="180" y="145" font-size="10">Feature C</text>
    <text x="150" y="190" text-anchor="middle" font-style="italic" font-size="12">&quot;Average behavior&quot;</text>

    <text x="450" y="30" font-family="Arial" font-size="16" font-weight="bold" text-anchor="middle" fill="#e74c3c">Local
      Interpretability</text>
    <rect x="350" y="50" width="200" height="150" fill="none" stroke="#e74c3c" stroke-width="2" rx="5" />

    <circle cx="450" cy="100" r="30" fill="#fadbd8" stroke="#e74c3c" />
    <text x="450" y="105" text-anchor="middle" font-weight="bold">Mr. Smith</text>

    <line x1="450" y1="130" x2="450" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrow)" />
    <text x="450" y="180" text-anchor="middle" font-weight="bold" fill="#c0392b">Denied</text>
    <text x="450" y="195" text-anchor="middle" font-size="10">&quot;Because Debt &gt; Income&quot;</text>

    <line x1="300" y1="50" x2="300" y2="250" stroke="#ccc" stroke-dasharray="5,5" />
  </svg>

  <hr>
  <h3 id="2-transparent-model-design-regularization">2. Transparent Model Design &amp; Regularization</h3>
  <p>How do we design models to be <strong>more</strong> interpretable? We use <strong>Regularization</strong>.</p>
  <p><strong>The Problem:</strong>
    A Linear Regression model might use all 1,000 features in a dataset, giving tiny weights (0.0001) to 900 of them.
    This is technically &quot;White-box,&quot; but impossible for a human to read.</p>
  <p><strong>The Solution: Sparse Models (LASSO)</strong>
    We want the model to force irrelevant feature weights to exactly <strong>Zero</strong>.</p>
  <h4 id="a-lasso-l1-regularization-">A. LASSO (L1 Regularization)</h4>
  <ul>
    <li><strong>Math:</strong> It adds a penalty term to the Loss Function equal to the <em>absolute value</em> of the
      coefficients.
      $$Loss = \sum (y - \hat{y})^2 + \lambda \sum |\beta_j|$$</li>
    <li><strong>Effect:</strong> The diamond-shaped geometry of the L1 penalty forces coefficients to hit exactly zero.
    </li>
    <li><strong>Result:</strong> Instead of 1,000 features, the model selects the top 10 features and ignores the rest.
    </li>
    <li><strong>Benefit:</strong> A model with 10 variables is highly <strong>Interpretable</strong>.</li>
  </ul>
  <h4 id="b-ridge-l2-regularization-">B. Ridge (L2 Regularization)</h4>
  <ul>
    <li><strong>Math:</strong> Adds a penalty equal to the <em>square</em> of coefficients ($\beta^2$).</li>
    <li><strong>Effect:</strong> It shrinks weights to be <em>small</em> (0.00001) but rarely <em>zero</em>.</li>
    <li><strong>Interpretability:</strong> Less useful for XAI than LASSO because it keeps all features in the model.
    </li>
  </ul>
  <hr>
  <h3 id="3-advantages-and-limitations-of-inherently-interpretable-models">3. Advantages and Limitations of Inherently
    Interpretable Models</h3>
  <p>We wrap up Unit II with a summary of these &quot;White-box&quot; models.</p>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Aspect</th>
        <th style="text-align:left">Advantages</th>
        <th style="text-align:left">Limitations</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Transparency</strong></td>
        <td style="text-align:left">Fully transparent logic. No &quot;Black Box&quot; magic.</td>
        <td style="text-align:left">Cannot model complex patterns (e.g., cannot recognize faces).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Trust</strong></td>
        <td style="text-align:left">High user trust. Doctors/Bankers prefer them.</td>
        <td style="text-align:left">Accuracy is usually lower than Deep Learning on big data.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Debugging</strong></td>
        <td style="text-align:left">Easy to spot errors (e.g., &quot;Why does Age have a negative weight?&quot;).</td>
        <td style="text-align:left">Performance suffers with high-dimensional data (curse of dimensionality).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Deployment</strong></td>
        <td style="text-align:left">Very fast and lightweight (just a math formula).</td>
        <td style="text-align:left">Requires heavy Feature Engineering (human effort) to work well.</td>
      </tr>
    </tbody>
  </table>
  <hr>
  <h3 id="4-exam-question-bank">4. Exam Question Bank</h3>
  <p><strong>Q1 (Short - 2 Marks): Differentiate between Global and Local Interpretability.</strong></p>
  <blockquote>
    <p><strong>Ans:</strong></p>
    <ul>
      <li><strong>Global:</strong> Explains the overall logic of the model across the entire population (e.g.,
        &quot;Does the model prioritize Age?&quot;).</li>
      <li><strong>Local:</strong> Explains the reasoning for a specific single prediction (e.g., &quot;Why was
        <em>this</em> image classified as a cat?&quot;).</li>
    </ul>
  </blockquote>
  <p><strong>Q2 (5 Marks): How does LASSO (L1 Regularization) improve model interpretability?</strong></p>
  <blockquote>
    <p><strong>Hint:</strong> Explain that LASSO acts as an automated <strong>Feature Selector</strong>. By forcing the
      weights of irrelevant features to exactly zero, it produces a <strong>Sparse Model</strong>. A model with fewer
      active features (sparsity) is easier for humans to understand (cognitive load is lower).</p>
  </blockquote>
  <p><strong>Q3 (10 Marks Unit Summary): &quot;White-box models are desirable but not always feasible.&quot;
      Discuss.</strong></p>
  <blockquote>
    <p><strong>Structure:</strong></p>
    <ol>
      <li><strong>Desirable:</strong> Because of Trust, Fairness, and Regulatory compliance (GDPR).</li>
      <li><strong>Not Feasible:</strong> When the data is unstructured (Images, Audio) or highly non-linear. A Decision
        Tree cannot drive a car.</li>
      <li><strong>Conclusion:</strong> We need White-box for tabular data (Banks), but Black-box + XAI for perceptual
        tasks (Vision).</li>
    </ol>
  </blockquote>
  <hr>
  <h2 id="topic-5-part-b-transparent-model-design-regularization">Topic 5 (Part B): Transparent Model Design &amp;
    Regularization</h2>
  <hr>
  <h3 id="1-the-philosophy-of-transparent-design">1. The Philosophy of Transparent Design</h3>
  <p><strong>Transparent Model Design</strong> is the practice of constraining a machine learning model <em>during the
      training phase</em> so that the resulting structure is simple enough for humans to understand.</p>
  <p>Instead of training a complex Black Box and trying to explain it later (Post-hoc), we force the model to be simple
    from the start.</p>
  <h4 id="key-strategies-for-transparent-design-">Key Strategies for Transparent Design:</h4>
  <ol>
    <li><strong>Sparsity:</strong> Limiting the number of features used. A model using 5 features is interpretable; a
      model using 500 is not.</li>
    <li><strong>Monotonicity Constraints:</strong> Forcing the model to obey logical rules.<ul>
        <li><em>Example:</em> We can force a Credit Scoring model so that <em>increasing</em> Income
          <strong>never</strong> decreases the Credit Score. This ensures the model behaves logically.</li>
      </ul>
    </li>
    <li><strong>Interaction Limits:</strong> Preventing the model from creating complex &quot;Feature A <em> Feature B
      </em> Feature C&quot; interactions that humans cannot grasp.</li>
  </ol>
  <hr>
  <h3 id="2-regularization-the-math-of-simplicity">2. Regularization: The Math of Simplicity</h3>
  <p><strong>Regularization</strong> is a technique used to prevent overfitting, but in XAI, we use it to <strong>force
      interpretability</strong>.</p>
  <p>The General Formula for a model with regularization:
    $$\text{Total Loss} = \text{Prediction Error} + \lambda \times \text{Complexity Penalty}$$</p>
  <ul>
    <li><strong>Prediction Error:</strong> How wrong the model is (MSE).</li>
    <li><strong>Complexity Penalty:</strong> A punishment for having too many features or large weights.</li>
    <li><strong>$\lambda$ (Lambda):</strong> The control knob.<ul>
        <li>High $\lambda$ = Simple, Interpretable Model (High Bias).</li>
        <li>Low $\lambda$ = Complex, Accurate Model (High Variance).</li>
      </ul>
    </li>
  </ul>
  <hr>
  <h3 id="3-l1-regularization-lasso-the-king-of-xai">3. L1 Regularization (LASSO) - The King of XAI</h3>
  <p><strong>LASSO</strong> (Least Absolute Shrinkage and Selection Operator) is the most important regularization
    technique for Explainable AI.</p>
  <h4 id="a-the-math">A. The Math</h4>
  <p>It adds the <strong>Absolute Value</strong> of coefficients as a penalty.
    $$\text{Loss} = \sum (y - \hat{y})^2 + \lambda \sum_{j=1}^{p} |\beta_j|$$</p>
  <h4 id="b-why-it-creates-interpretability-sparsity-">B. Why it creates Interpretability (Sparsity)</h4>
  <p>LASSO has a unique mathematical property: it forces the coefficients of irrelevant features to become
    <strong>exactly zero</strong>.</p>
  <ul>
    <li><strong>Feature Selection:</strong> If you have 1000 features, LASSO might set 990 of their weights to $0$.</li>
    <li><strong>Result:</strong> You are left with a model utilizing only the 10 most important features.</li>
    <li><strong>XAI Benefit:</strong> It removes the &quot;noise,&quot; leaving only the &quot;signal&quot; for the
      human to analyze.</li>
  </ul>
  <h4 id="c-visual-geometry-the-diamond-vs-the-circle-">C. Visual Geometry (The &quot;Diamond&quot; vs. The
    &quot;Circle&quot;)</h4>
  <p>This is a classic exam explanation.</p>
  <ul>
    <li><strong>L1 (LASSO)</strong> creates a diamond-shaped constraint region. The optimal solution often hits the
      &quot;corner&quot; of the diamond (where the axis is 0).</li>
    <li><strong>L2 (Ridge)</strong> creates a circular constraint. The solution hits the edge of the circle, shrinking
      the weight close to 0, but rarely <em>exactly</em> 0.</li>
  </ul>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <text x="300" y="30" font-family="Arial" font-size="16" font-weight="bold" text-anchor="middle">Why L1 (LASSO)
      creates Zero Weights</text>

    <g transform="translate(150, 160)">
      <line x1="-100" y1="0" x2="100" y2="0" stroke="#333" />
      <line x1="0" y1="-100" x2="0" y2="100" stroke="#333" />

      <polygon points="0,-60 60,0 0,60 -60,0" fill="#e3f2fd" stroke="#2196f3" stroke-width="2" />

      <ellipse cx="70" cy="70" rx="30" ry="15" fill="none" stroke="#e74c3c" stroke-width="2"
        transform="rotate(-45 70 70)" />
      <ellipse cx="70" cy="70" rx="50" ry="25" fill="none" stroke="#e74c3c" stroke-width="2"
        transform="rotate(-45 70 70)" />
      <ellipse cx="70" cy="70" rx="80" ry="40" fill="none" stroke="#e74c3c" stroke-width="2"
        transform="rotate(-45 70 70)" />

      <circle cx="60" cy="0" r="5" fill="#2c3e50" />
      <text x="-50" y="-80" font-weight="bold" fill="#2196f3">L1 (Diamond)</text>
      <text x="70" y="-10" font-size="10">Intersection at Axis!</text>
      <text x="70" y="5" font-size="10">(Weight 2 = 0)</text>
    </g>

    <g transform="translate(450, 160)">
      <line x1="-100" y1="0" x2="100" y2="0" stroke="#333" />
      <line x1="0" y1="-100" x2="0" y2="100" stroke="#333" />

      <circle cx="0" cy="0" r="60" fill="#e8f5e9" stroke="#4caf50" stroke-width="2" />

      <ellipse cx="70" cy="70" rx="30" ry="15" fill="none" stroke="#e74c3c" stroke-width="2"
        transform="rotate(-45 70 70)" />
      <ellipse cx="70" cy="70" rx="50" ry="25" fill="none" stroke="#e74c3c" stroke-width="2"
        transform="rotate(-45 70 70)" />

      <circle cx="42" cy="42" r="5" fill="#2c3e50" />
      <text x="-50" y="-80" font-weight="bold" fill="#4caf50">L2 (Circle)</text>
      <text x="50" y="30" font-size="10">Intersection NOT at Axis</text>
      <text x="50" y="45" font-size="10">(Both weights non-zero)</text>
    </g>
  </svg>

  <hr>
  <h3 id="4-l2-regularization-ridge-">4. L2 Regularization (Ridge)</h3>
  <p><strong>Ridge Regression</strong> is less useful for transparency but good for stability.</p>
  <ul>
    <li><strong>The Math:</strong> Adds the <strong>Squared Value</strong> of coefficients.
      $$\text{Loss} = \sum (y - \hat{y})^2 + \lambda \sum_{j=1}^{p} \beta_j^2$$</li>
    <li><strong>Effect:</strong> It punishes large weights. It makes all coefficients <em>small</em>, but it keeps all
      of them.</li>
    <li><strong>XAI Consequence:</strong> You end up with a &quot;Dense&quot; model. If you have 100 features, you still
      have 100 non-zero coefficients. This is hard to interpret.</li>
  </ul>
  <hr>
  <h3 id="5-comparison-for-exam-10-marks-">5. Comparison for Exam (10 Marks)</h3>
  <table>
    <thead>
      <tr>
        <th style="text-align:left">Feature</th>
        <th style="text-align:left">L1 Regularization (LASSO)</th>
        <th style="text-align:left">L2 Regularization (Ridge)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:left"><strong>Penalty Term</strong></td>
        <td style="text-align:left">Absolute Value ($</td>
        <td style="text-align:left">\beta</td>
        <td>$)</td>
        <td>Squared Value ($\beta^2$)</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Geometry</strong></td>
        <td style="text-align:left">Diamond Shape</td>
        <td style="text-align:left">Circular Shape</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Effect on Weights</strong></td>
        <td style="text-align:left">Forces many to <strong>Exactly Zero</strong>.</td>
        <td style="text-align:left">Shrinks them to be <strong>Very Small</strong>.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Resulting Model</strong></td>
        <td style="text-align:left"><strong>Sparse</strong> (Few features).</td>
        <td style="text-align:left"><strong>Dense</strong> (All features kept).</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Interpretability</strong></td>
        <td style="text-align:left"><strong>High.</strong> Acts as automated feature selection.</td>
        <td style="text-align:left"><strong>Low.</strong> Hard to explain 1000 tiny weights.</td>
      </tr>
      <tr>
        <td style="text-align:left"><strong>Best For</strong></td>
        <td style="text-align:left">XAI &amp; Feature Selection.</td>
        <td style="text-align:left">Preventing Overfitting when all features matter.</td>
      </tr>
    </tbody>
  </table>
  <hr>
  <h3 id="6-exam-question-bank">6. Exam Question Bank</h3>
  <p><strong>Q1 (Applied - 5 Marks): You are building a credit scoring model with 500 potential variables (age, income,
      shoe size, favorite color...). You need to explain the model to bank regulators. Which regularization technique
      should you use and why?</strong></p>
  <blockquote>
    <p><strong>Answer:</strong> You should use <strong>L1 Regularization (LASSO)</strong>.</p>
    <ul>
      <li><strong>Reason:</strong> LASSO forces the coefficients of irrelevant variables (like shoe size, favorite
        color) to exactly zero.</li>
      <li><strong>Outcome:</strong> The final model will likely select only the top ~10 relevant variables (Income,
        Debt).</li>
      <li><strong>Benefit:</strong> It is much easier to explain a 10-variable equation to a regulator than a
        500-variable equation where most variables have tiny, confusing effects.</li>
    </ul>
  </blockquote>
  <p><strong>Q2 (Theory - 5 Marks): Explain the concept of &quot;Sparsity&quot; in the context of XAI.</strong></p>
  <blockquote>
    <p><strong>Answer:</strong> Sparsity refers to a matrix or model where the majority of elements are zero. In XAI, a
      &quot;Sparse Model&quot; is one that uses very few input features to make a decision. This minimizes the cognitive
      load on the human user, making the model transparent and interpretable.</p>
  </blockquote>
  <hr>
  <h2 id="topic-6-advantages-and-limitations-of-inherently-interpretable-models">Topic 6: Advantages and Limitations of
    Inherently Interpretable Models</h2>
  <hr>
  <h3 id="1-introduction">1. Introduction</h3>
  <p><strong>Inherently Interpretable Models</strong> (also known as &quot;White-box&quot; or &quot;Glass-box&quot;
    models) are algorithms where the relationship between input and output is transparent by design.</p>
  <ul>
    <li><strong>Examples:</strong> Linear Regression, Logistic Regression, Decision Trees, Rule-based Systems (OneR).
    </li>
    <li><strong>Core Characteristic:</strong> The explanation is <strong>intrinsic</strong>. We do not need a separate
      tool to understand them; the model structure <em>is</em> the explanation.</li>
  </ul>
  <p>While these models are the gold standard for transparency, they are not a silver bullet for all AI problems.</p>
  <hr>
  <h3 id="2-advantages-the-white-box-promise-">2. Advantages (The &quot;White-box&quot; Promise)</h3>
  <p>If asked &quot;Why should we prefer simple models?&quot;, cite these points:</p>
  <h4 id="a-total-transparency-trust">A. Total Transparency &amp; Trust</h4>
  <ul>
    <li><strong>Logic:</strong> The decision logic is explicit. In a Decision Tree, you can see the path. In Linear
      Regression, you see the weight.</li>
    <li><strong>Impact:</strong> This builds immediate trust with domain experts. A doctor can look at a decision tree
      and verify if it aligns with medical knowledge.</li>
    <li><strong>Regulation:</strong> They are the easiest way to comply with <strong>GDPR Article 22</strong>
      (&quot;Right to Explanation&quot;).</li>
  </ul>
  <h4 id="b-ease-of-debugging">B. Ease of Debugging</h4>
  <ul>
    <li><strong>The Scenario:</strong> If the model makes a mistake (e.g., predicting a house price as negative).</li>
    <li><strong>White-box:</strong> You can look at the weights. &quot;Oh, the weight for <code>Garage_Size</code> is
      negative -1000. That&#39;s a bug.&quot;</li>
    <li><strong>Black-box:</strong> You are staring at a matrix of 10 million numbers. Good luck finding the error.</li>
  </ul>
  <h4 id="c-data-efficiency">C. Data Efficiency</h4>
  <ul>
    <li>These models generally require <strong>less training data</strong> to converge compared to Deep Neural Networks,
      which are data-hungry.</li>
  </ul>
  <h4 id="d-computational-speed">D. Computational Speed</h4>
  <ul>
    <li><strong>Training:</strong> Very fast (seconds to minutes).</li>
    <li><strong>Inference:</strong> Instantaneous. A linear equation $y=mx+c$ takes nanoseconds to compute. Ideally
      suited for edge devices (IoT) with low power.</li>
  </ul>
  <hr>
  <h3 id="3-limitations-the-performance-gap-">3. Limitations (The Performance Gap)</h3>
  <p>Why don&#39;t we use Decision Trees for everything?</p>
  <h4 id="a-the-accuracy-interpretability-trade-off">A. The Accuracy-Interpretability Trade-off</h4>
  <ul>
    <li><strong>The Problem:</strong> Real-world data is messy and non-linear.</li>
    <li><strong>The Failure:</strong> A straight line (Linear Regression) cannot fit a curve. A simple decision tree
      cannot capture complex, subtle patterns without becoming a massive, unreadable bush.</li>
    <li><strong>Result:</strong> On complex tasks, White-box models often have <strong>lower accuracy</strong> than
      Black-box models (like Gradient Boosting or Deep Learning).</li>
  </ul>
  <h4 id="b-inability-to-handle-perceptual-data">B. Inability to Handle &quot;Perceptual&quot; Data</h4>
  <ul>
    <li><strong>Images/Audio:</strong> You cannot use a Decision Tree to classify an image of a cat.<ul>
        <li><em>Why?</em> An image has 784+ pixels. A rule like <code>IF Pixel_123 &gt; 50 THEN Cat</code> makes no
          sense.</li>
        <li>Deep Learning (Black-box) is required for Vision, Speech, and NLP.</li>
      </ul>
    </li>
  </ul>
  <h4 id="c-the-feature-engineering-burden">C. The Feature Engineering Burden</h4>
  <ul>
    <li><strong>White-box:</strong> Requires human experts to hand-craft features (e.g., creating a &quot;BMI&quot;
      feature from &quot;Height&quot; and &quot;Weight&quot;).</li>
    <li><strong>Black-box:</strong> Deep Learning does <strong>Feature Learning</strong> automatically. It figures out
      that BMI matters on its own.</li>
  </ul>
  <h4 id="d-the-curse-of-dimensionality">D. The Curse of Dimensionality</h4>
  <ul>
    <li>As the number of features grows (e.g., to 10,000), Linear Models become unstable (Multicollinearity) and
      Decision Trees become prone to overfitting.</li>
  </ul>
  <hr>
  <h3 id="4-visual-summary-the-scale-of-balance">4. Visual Summary: The Scale of Balance</h3>
  <svg width="100%" height="300" xmlns="http://www.w3.org/2000/svg"
    style="background:#fff; border: 1px solid #ddd; border-radius: 8px;">
    <polygon points="250,250 350,250 300,150" fill="#2c3e50" />
    <line x1="100" y1="150" x2="500" y2="150" stroke="#2c3e50" stroke-width="4" />

    <line x1="100" y1="150" x2="100" y2="200" stroke="#333" stroke-width="2" />
    <path d="M50,200 Q100,250 150,200" fill="#e3f2fd" stroke="#2196f3" stroke-width="2" />
    <text x="100" y="225" text-anchor="middle" font-weight="bold" fill="#0d47a1">White Box</text>
    <text x="100" y="240" text-anchor="middle" font-size="10" fill="#0d47a1">High Trust</text>
    <text x="100" y="255" text-anchor="middle" font-size="10" fill="#0d47a1">Easy Debugging</text>

    <line x1="500" y1="150" x2="500" y2="200" stroke="#333" stroke-width="2" />
    <path d="M450,200 Q500,250 550,200" fill="#2c3e50" stroke="#000" stroke-width="2" />
    <text x="500" y="225" text-anchor="middle" font-weight="bold" fill="#000000">Black Box</text>
    <text x="500" y="240" text-anchor="middle" font-size="10">High Accuracy</text>
    <text x="500" y="255" text-anchor="middle" font-size="10">Auto-Features</text>

    <text x="300" y="100" text-anchor="middle" font-style="italic" fill="#555">The Eternal Trade-off</text>
  </svg>

  <hr>
  <h3 id="5-exam-question-bank">5. Exam Question Bank</h3>
  <p><strong>Q1 (10 Marks): &quot;In the medical domain, inherently interpretable models are often preferred over
      high-accuracy black-box models.&quot; Discuss the advantages and limitations that justify this statement.</strong>
  </p>
  <ul>
    <li><strong>Structure of Answer:</strong>
      <ol>
        <li><strong>Context:</strong> In medicine, the cost of a mistake is life or death.</li>
        <li><strong>Advantages (Why preferred?):</strong>
          <ul>
            <li><strong>Trust:</strong> Doctors need to know <em>why</em> a diagnosis was made.</li>
            <li><strong>Causality:</strong> Determining if a risk factor (e.g., smoking) actually causes the disease.
            </li>
            <li><strong>Legal:</strong> Malpractice liability requires explanation.</li>
          </ul>
        </li>
        <li><strong>Limitations (The downside):</strong>
          <ul>
            <li>They might miss subtle patterns in complex genomic data or MRI scans (where Deep Learning is better).
            </li>
          </ul>
        </li>
        <li><strong>Conclusion:</strong> We sacrifice a small amount of accuracy for a large amount of safety/trust.
        </li>
      </ol>
    </li>
  </ul>
  <p><strong>Q2 (5 Marks): Why are White-box models unsuitable for Image Recognition?</strong></p>
  <ul>
    <li><strong>Key Points:</strong>
      <ul>
        <li>Images are high-dimensional (millions of pixel values).</li>
        <li>Pixel-level rules (<code>If Pixel 50 is Red...</code>) are meaningless semantically.</li>
        <li>White-box models cannot capture <strong>invariance</strong> (a cat is still a cat if it rotates). Deep
          Learning (CNNs) is required for this.</li>
      </ul>
    </li>
  </ul>
  <hr>
</body>

</html>